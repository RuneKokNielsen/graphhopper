\documentclass{article}
\setlength{\parindent}{0ex}
\setlength{\parskip}{1em}
\usepackage[utf8]{inputenc} 
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\renewenvironment{proof}[1][\proofname]{{\bfseries #1.}}{\qed}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{fancybox}
\usepackage{tikz}
\usepackage{tkz-euclide}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{qtree}
\usepackage{scrextend}
\usepackage{multirow}
\usepackage{float}
\usepackage{algpseudocode}
\usepackage[bottom]{footmisc}
\usepackage[toc,page]{appendix}
\usepackage{pbox}


\tikzset{main node/.style={circle,fill=blue!20,draw,minimum size=1cm,inner sep=0pt},
}


%Kodestyling \begin{lstlisting}
\usepackage{color}
\usepackage{listings}
\lstset{ %
language=C++,                % choose the language of the code
%basicstyle=\footnotesize,       % the size of the fonts that are used for the code
basicstyle=\ttfamily,
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,           % adds a frame around the code
tabsize=2,          % sets default tabsize to 2 spaces
captionpos=b,           % sets the caption-position to bottom
breaklines=true,        % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)},          % if you want to add a comment within your code
mathescape
}

\def\multiset#1#2{\ensuremath{\left(\kern-.3em\left(\genfrac{}{}{0pt}{}{#1}{#2}\right)\kern-.3em\right)}}

\usepackage{caption}
\captionsetup{font=small}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\def\meta#1{\mbox{$\langle\hbox{#1}\rangle$}}
\def\macrowitharg#1#2{{\tt\string#1\bra\meta{#2}\ket}}

{\escapechar-1 \xdef\bra{\string\{}\xdef\ket{\string\}}}

\def\intro#1{{#1}{\cal I}}
\def\elim#1{{#1}{\cal E}}

\showboxbreadth 999
\showboxdepth 999
\tracingoutput 1


\let\imp\to
\def\elim#1{{{#1}{\cal E}}}
\def\intro#1{{{#1}{\cal I}}}
\def\lt{<}
\def\eqdef{=}
\def\eps{\mathrel{\epsilon}}
\def\biimplies{\leftrightarrow}
\def\flt#1{\mathrel{{#1}^\flat}}
\def\setof#1{{\left\{{#1}\right\}}}
\let\implies\to
\def\KK{{\mathsf K}}
\let\squashmuskip\relax

\graphicspath{ {images/} }
\usetikzlibrary{arrows}
\tikzset{
  leaf_/.style = {shape=rectangle,draw, align=center},
  node_/.style     = {shape=circle,draw,align=center}
}
\author{Rune Kok Nielsen (qkd362), Andreas Nugaard Holm (jnh508)}
\title{GraphHopper kernel in C++}
\DeclareMathOperator{\Ran}{Ran}
\DeclareMathOperator{\Dom}{Dom}

\renewcommand*\contentsname{Contents}
\begin{document}
	
\maketitle
\newpage
\tableofcontents
\newpage

\section{Introduction}
This report presents our work and the theory used developing and testing an efficient implementation of the GraphHopper kernel with \textit{Gappy paths} in C++ for our Bachelor thesis. While we do introduce the basics behind SVM and kernels based on our earlier work in \cite{svm-graph-kernels} the focus will primarily lie on the algorithm behind the kernel itself as well as the introduction of \textit{Gaps}. Gaps have been proved useful in string kernels but have to our knowledge never before been implemented in a graph kernel. After presenting the theory we will provide the results of our experiments analysing the time efficiency of our implementation compared to an existing MATLAB-implementation as well as the effect on classification accuracy in different datasets when adding \textit{Gaps} to the kernel.

\section{SVM}
Before introducing the GraphHopper kernel we need to establish some essential underlying theory. In this section we will focus on the basic theory of classification, SVM, kernels and cross-validation. This is a condensed summary of the most relevant parts of our former work \cite{svm-graph-kernels} where we discuss the topics thoroughly.

\subsection{Classification}
The core problem is data classification, i.e.\ determining the class of some data point from a limited number of predefined classes. In the context of this report we will be working with binary classification of graphs using SVM.

\subsection{SVM}
SVM is a binary classification technique used to create classification models. The technique works by placing a hyperplane between two optimal data points in the training set (\textit{the support vectors}) such that the hyperplane successfully separates the data points by their classes as far possible. Such a hyperplane in a 2-dimensional, linear seperable case is illustrated in figure \ref{fig:hyperplane}. The accuracy of the model is then measured by how successfully the hyperplane classifies new data points with known classes.

\begin{figure}[H]
	\centering
	\includegraphics[width=10cm]{svm_plot}
	\caption{\textit{B1 is the hyperplane which splits the data points by their classes. The planes b11 and b12 represent the margins of B1 and intersects their respective support vectors.}}
	\label{fig:hyperplane}
\end{figure}

The challenge of finding the optimal hyperplane is solved by maximizing the dual Lagrange optimization problem
\begin{equation}
\label{eq:dual_lagrange_problem}
L_D = \sum_{i=1}^{n}\lambda_i-\frac{1}{2}\sum_{i,j}\lambda_i\lambda_j y_iy_j x_ix_j
\end{equation}
where $x_i$ is a data point, $y_i\in \{-1, 1\}$ is its class and $\lambda_i$ is its Lagrange multiplier.

\subsection{Kernels}
The data illustrated in figure \ref{fig:hyperplane} is linearly separable, i.e.\ may be separated by a hyperplane. However, this is not at all the case for all data. When working with linearly inseparable data we may, however, transform the data points into some $x\leq\infty$-dimensional space such that the transformed data points become linearly separable. The optimization problem is then simply changed to
\begin{equation}
\label{eq:kernel_function_phi}
L_D = \sum_{i=1}^{n}\lambda_i-\frac{1}{2}\sum_{i,j}\lambda_i\lambda_j y_iy_j\Phi(x_i)\cdot\Phi(x_j)
\end{equation}
where $\Phi:\chi_1\rightarrow\chi_2$ is some transformation that (hopefully) makes the data somewhat more separable.

Finding this transformation $\Phi$ is, however, a daunting task (let alone transforming all the data). We work around this issue by using the \textit{kernel trick} based on Mercer's theorem.

\textbf{Mercer's theorem}\\
\textit{Let $k:\chi_1\times\chi_1\rightarrow\mathbb{R}$ be a symmetric, positive semidefinite\cite{svn-introduction} function. Then there exists some transformation $\Phi:\chi_1\rightarrow\chi_2$ such that}
\begin{equation}
\langle\Phi(x_i),\Phi(x_j)\rangle=k(x_i,x_j)
\end{equation}

We supply a proof for this theorem in \cite{svm-graph-kernels}.

Using this knowledge we may now define a function $k(x_i,x_j)$ with the given properties and substitute this function (called a kernel function, or just kernel) into the optimization problem as seen in (\ref{eq:kernel_function}). This substitution is known as the kernel trick.

\begin{equation}
\label{eq:kernel_function}
L_D = \sum_{i=1}^{n}\lambda_i-\frac{1}{2}\sum_{i,j}\lambda_i\lambda_j y_iy_jk(x_i,x_j)
\end{equation}

The task is then to find a kernel that manages to compare the features of the data in a meaningful way making them separable in the transformed space. The GraphHopper kernel attempts to do just that by analysing a combination of structure and node labels in graphs. As it is defined exclusively for graphs it belongs to a subgroup of kernels aptly named graph kernels.

Assuming a kernel function $k(x_i,x_j)$ we define the matrix $K$ to be an $N\times N$ matrix (where $N$ is the number of data points) such that $K_{ij}=k(x_i,x_j)$. Using SVM in practice then consists of computing $K$ and creating a model through SVM based on the result.

\subsection{Cross-validation}
There may be several parameters involved when using SVM to create a classification model. To properly select these parameters and realistically predict the resulting model's accuracy we take advantage of cross-validation.

To cross-validate parameters and accuracy we split the dataset $M$ into $p$ parts. We then iterate $p$ times each time selecting different combinations of a test, validation and training set $Test_i, Val_i, Train_i\subset M$ such that $|Test_i|=|Val_i|=\frac{|M|}{p}$,  $|Train_i| = \frac{|M| * (p-2)}{p}$ and such that there is no overlap between any of the sets (see figure). In each iteration $i$ we train a model for all combinations of parameters using the training set $Train_i$ and estimate their accuracy by predicting on the validation set $Val_i$. We then select the combination of parameters that gave the highest accuracy and train a new model using $Train_i\cup Val_i$ with these parameters and finally estimate its accuracy by predicting on the test set $Test_i$ saving the resulting accuracy as $acc_i$. After $i$ iterations we estimate the expected accuracy to be the average accuracy achieved, i.e. $\frac{acc_1 + acc_2 + ... + acc_i}{i}$.


\begin{minipage}{0.7\linewidth}
	To cross-validate parameters and accuracy we split the dataset $M$ into $p$ parts. We then iterate $p$ times each time selecting different combinations of a test, validation and training set $Test_i, Val_i, Train_i\subset M$ such that $|Test_i|=|Val_i|=\frac{|M|}{p}$,  $|Train_i| = \frac{|M| * (p-2)}{p}$ and such that there is no overlap between any of the sets (figure \ref{fig:cross_validation}). In each iteration $i$ we train a model for all combinations of parameters using the training set $Train_i$ and estimate their accuracy by predicting on the validation set $Val_i$. We then select the combination of parameters that gave the highest accuracy and train a new model using $Train_i\cup Val_i$ with these parameters and finally estimate its accuracy by predicting on the test set $Test_i$ saving the resulting accuracy as $acc_i$. After $i$ iterations we estimate the expected accuracy to be the average accuracy achieved, i.e. $\frac{acc_1 + acc_2 + ... + acc_i}{i}$.
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[c]{0.3\linewidth}
	\centering
		\includegraphics[width=3cm]{10-fold-cross-validation_3}
		\captionof{figure}{Each iteration selects a new combination of subsets.}
		\label{fig:cross_validation}
\end{minipage}

\section{GraphHopper}
\label{section:graphhopper}
The following description of the GraphHopper kernel is based on the presentation in \cite{graphhopper}.

GraphHopper is a graph kernel, i.e.\ a kernel for comparing graph data. The kernel effectively analyses the structure of the graphs by comparing shortest paths similarly to \textit{shortest path kernel}\footnote{We give a detailed description of the shortest path kernel in \cite{svm-graph-kernels}.} (SPK)\cite{shortest-path}. An advantage of these kernels is that they may analyse the structure of the graphs along with the properties of their nodes.
While SPK compares all shortest paths with a complexity of $\mathcal{O}(n^4)$, where $n$ is the number of nodes, GraphHopper only compares shortest paths of equal length with complexity $\mathcal{O}(n^2(m+\log n+\delta^2 + d))$ where $m$ is the number of edges, $\delta$ is the diameter of the graph and $d$ is the dimension of the node attributes. The complexity will be explained along with the algorithm itself.

Let $G=(V,E)$ and $G'=(V',E')$ be graphs such that $V,V'$ are nodes and $E,E'$ are edges and let $P,P'$ be the sets of all shortest paths in $G$ and $G'$ respectively. The kernel is then a sum of path kernel $k_p$ over each pair of shortest paths $\pi\in P,\ \pi'\in P'$:

\begin{equation}
\label{eq:k}
k_{graphhopper}(G,G')=\sum_{\pi\in P}\sum_{\pi'\in P'}k_p(\pi, \pi')
\end{equation}
The path kernel $k_p$ is a sum of some node kernel $k_n$ on each pair of nodes $v\in\pi,\ v'\in\pi'$ such that $v$ and $v'$ appears at the same place in $\pi$ and $\pi'$ for pairs $\pi$ and $\pi'$ of equal discrete length\footnote{Discrete length meaning that $\pi$ and $\pi'$ contains the same number of nodes.}. I.e.
\begin{equation}
\label{eq:k_p}
k_p(\pi, \pi')=\begin{cases}
\sum_{i=1}^{|\pi|}k_n(\pi(i), \pi'(i)) & \text{for }|\pi|=|\pi'|\\
0 & \text{otherwise}
\end{cases}
\end{equation}
where $\pi(i)$ denotes the $i$'th node in $\pi$. The node kernels that we have chosen to implement are all described in a later section and are not essential in describing the kernel itself.

Using the definition above we now present a simple example. Consider the two graphs, $G$ and $G'$ in figure \ref{fig:shortest-path-graph}.


\begin{figure}[H]
	\begin{minipage}[t]{0.7\linewidth}
		\begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=4cm,
		thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
		
		\node[main node] (a) [label=a] {};
		\node[main node] (b) [below of=a, label=b] {};
		\node[main node] (c) [right of=a, label=c] {};
		
		
		\path[every node/.style={font=\sffamily\small}]
		(a) edge [bend right] node {}(b)
		(a) edge node {}(c)
		;
		\end{tikzpicture}
	\end{minipage}
	\begin{minipage}[t]{0.2\linewidth}
		\begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=4cm,
		thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
		
		
		\node[main node] (d) [label=d] {};
		\node[main node] (e) [below of=d, label=e] {};
		
		
		\path[every node/.style={font=\sffamily\small}]
		(d) edge [bend right] node {}(e)
		;
		\end{tikzpicture}
	\end{minipage}
	\caption{\textit{Left: G. Right: G'}}
	\label{fig:shortest-path-graph}
\end{figure}
Identifying shortest paths we get
\begin{align*}
P&=\{[a], [a,b], [a,c], [b], [b,a], [b,a,c], [c], [c,a], [c,a,b]\}\\
P'&=\{[d],[d,e],[e],[e,d]\}
\end{align*}
We can now compute $k_{graphhopper}$ by summing over all paths $\pi\in P$ and $\pi'\in P'$ such that $|\pi|=|\pi'|$. E.g. the pair $\pi=[a]\in P,\ \pi'=[d]\in P'$ are of equal discrete length so we add $k_n(a, d)$ to the sum. Identifying more pairs we find, for instance, that $|\pi|=|\pi'|$ for $\pi=[a,b]\in P,\ \pi'=[d,e]\in P'$ so we add $k_n(a,d) + k_n(b, e)$. For completeness of the example the entire sum can be seen in (\ref{eq:example_sum}).

\begin{equation}
\label{eq:example_sum}
\begin{split}
k_{graphhopper}(G,G')	&=  \underbrace{k_n(a,d)}_{k_p([a],[d])} + \underbrace{k_n(a, e)}_{k_p([a],[e])} + \underbrace{k_n(a, d) + k_n(b, e)}_{k_p([a,b],[d,e])} + \underbrace{k_n(a, e) + k_n(b, d)}_{k_p([a,b],[e,d])} \\ 
				&+ \underbrace{k_n(a, d) + k_n(c, e)}_{k_p([a,c],[d,e])} + \underbrace{k_n(a, e) + k_n(c, d)}_{k_p([a,c],[e,d])} + \underbrace{k_n(b, d)}_{k_p([b],[d])} + \underbrace{k_n(b, e)}_{k_p([b],[e])} \\
				&+ \underbrace{k_n(b, d) + k_n(a, e)}_{k_p([b,a],[d,e])} + \underbrace{k_n(b, e) + k_n(a, d)}_{k_p([b,a],[e,d])} + \underbrace{k_n(c, d)}_{k_p([c],[d])} + \underbrace{k_n(c, e)}_{k_p([c],[e])} \\
				&+ \underbrace{k_n(c, e) + k_n(a, d)}_{k_p([c,a],[d,e])}+ \underbrace{k_n(c, d) + k_n(a, e)}_{k_p([c,a],[e,d])}
\end{split}
\end{equation}

Observe how we added $k_n(a,d)$ multiple times to the final sum. Naturally, the number of times $k_n(v, v')$ will be added (where $v\in V,\ v'\in V'$) will be exactly the number of pairs of paths $\pi\in P,\ \pi'\in P'$ such that $|\pi|=|\pi'|$ and $\pi(i)=v,\ \pi'(i)=v'$ for some $i$.

Let $w(v, v')$ be the number of times we have to add $k_n(v, v')$ for some pair $v\in V,\ v'\in V'$ as described above. Instead of computing and adding $k_n(v, v')$ repeatedly we may instead add $w(v,v') k_n(v, v')$ once. We call $w(v, v')$ the $\textit{weight}$ between the two nodes and rewrite the kernel as a weighted sum between pairs of nodes:
\begin{equation}
\label{eq:k_graphhopper}
k_{graphhopper}(G,G')=\sum_{v\in V}\sum_{v'\in V'}w(v,v')k_n(v,v')
\end{equation}


When calculating $w(v,v')$ we sum the number of pairs $\pi,\pi'$ such that $|\pi|=|\pi'|$ and $\pi(i)=\pi'(i)$ for some $i$. Let $width(G)$ be the discrete length of the longest shortest path $\pi\in P$ and let $\delta=min(width(G), width(G'))$. Since we only consider pairs of paths of same length and the longest shortest path we do not have to consider any paths of length greater than $\delta$.

\iffalse
\begin{proof}[Proof: We may ignore paths of length greather than $\delta$]\\
Consider any two graphs $G,\ G'$ such that $width(G)=\delta$. Then we either have $\delta=width(G')$ or $\delta<width(G')$.\\
Let $width(G)=\delta=width(G')$ and let $\pi'\in P'$ such that $|\pi'|>\delta$. Since $G'$ is at least as wide as any shortest paths within it we have $width(G')\geq |\pi'|>\delta = width(G')$ which is an absurdity.\\
Let $width(G)=\delta < width(G')$ and let $\pi'\in P'$ such that $|\pi'|>\delta$. Now assume that there exists some $\pi\in P$ such that $|\pi|=|\pi'|$. We now see that $width(G)\geq |\pi| = |\pi'|>\delta=width(G)$ which is an absurdity.\\
We have now proven that there can not exist any pairs of paths $\pi\in P,\ \pi'\in P'$ such that $|\pi|=|\pi'|>\delta$.
\end{proof}
\fi

Beside from this limitation it is trivial that for a pair of paths $\pi,\ \pi'$ such that $|\pi|=|\pi'|=j$ we only have to consider nodes at index $i=1..j$ since there cannot exist any nodes at index $i>j$.


We can now formalize $w(v,v')$ as
\begin{equation}
w(v,v')=\sum_{j=1}^{\delta}\sum_{i=1}^{j}|\{(\pi, \pi')\ |\ \pi(i)=v,\ \pi'(i)=v',\ |\pi|=|\pi'|=j\}|
\end{equation}


Let $\delta=width(G)$ and let $M$ be a $|V|\times\delta\times\delta$ matrix such that $M(v)_{ij}$ is the number of shortest paths $\pi\in P$ such that $|\pi|=j$ and $\pi(i)=v$. We then have
\begin{equation}
w(v,v')=\sum_{j=1}^{\delta}\sum_{i=1}^{j}M(v)_{ij}*M'(v')_{ij}
\label{eq:wmm}
\end{equation}
allowing us to easily find the weight between pairs of nodes by first computing $M$ and $M'$. Furthermore, when computing $K$ for multiple graphs we may compute all M matrices beforehand and use them in the pairwise computations instead of computing them for each pair of graphs.


\subsection{Computing $M$}
The $M$ matrix can be computed in a number of ways. We will be using the message-passing algorithms presented in \cite{graphhopper}

Let $\tilde{v}$ be a node in $V$ and let $P_{\tilde{v}}$ be the set of shortest paths in $G$ starting in $\tilde{v}$. Then $M(v)_{ij}$ becomes $\sum_{\tilde{v}\in V}$ number of times $v$ appears as the $i$'th node in each $\pi_{\tilde{v}}\in P_{\tilde{v}}$ of discrete length $j$. This means that we can compute $M$ by iterating over each node $\tilde{v}\in V$ and finding all shortest paths starting in $\tilde{v}$.

Before we move on, we will first introduce the shortest path DAG $G_{\tilde{v}}=(V, E_{\tilde{v}})$ for $\tilde{v}$ where $E_{\tilde{v}}$ are edges kept in $G_{\tilde{v}}$. The shortest path DAG $G_{\tilde{v}}$ is created by removing a minimum amount of edges from $G$ so that it only contains edges that are part of some shortest path starting in $\tilde{v}$. We call $\tilde{v}$ the root node in the resulting DAG. This is illustrated in figure \ref{fig:shortest-path-dag} where we present a graph $G$ and the shortest path DAG $G_{a}$ rooted in $a$.

\begin{figure}[H]
	\begin{minipage}[t]{0.7\linewidth}
		\begin{tikzpicture}[-,>=stealth',shorten >=1pt,auto,node distance=4cm,
		thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
		
		\node[main node] (a) [label=a] {};
		\node[main node] (b) [below of=a, label=b] {};
		\node[main node] (c) [right of=a, label=c] {};
		\node[main node] (d) [below of=c, label=d] {};
		
		
		\path[every node/.style={font=\sffamily\small}]
		(a) edge [bend right] node {2}(b)
		(a) edge node {3}(c)
		(a) edge [bend right] node {1}(d)
		(d) edge [bend right] node {2}(c)
		(b) edge node {2}(d)
		(b) edge [bend right] node {3}(c)
		;
		\end{tikzpicture}
	\end{minipage}
	\begin{minipage}[t]{0.2\linewidth}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
		thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
		
		
		\node[main node] (a) [label=a] {};
		\node[main node] (b) [below of=a, label=b] {};
		\node[main node] (c) [right of=a, label=c] {};
		\node[main node] (d) [below of=c, label=d] {};
		
		
		\path[every node/.style={font=\sffamily\small}]
		(a) edge [bend right] node {2}(b)
		(a) edge node {3}(c)
		(a) edge [bend right] node {1}(d)
		(d) edge [bend right] node {2}(c)
		;
		\end{tikzpicture}
	\end{minipage}
	\caption{\textit{Left: G. Right: $G_{\tilde{a}}$}}
	\label{fig:shortest-path-dag}
\end{figure}

We now introduce two more constructs used for computing $M$: $\mathfrak{O}_{\tilde{v}}$ and $\mathfrak{D}_{\tilde{v}}$. 

\subsubsection{Computing $\mathfrak{O}_{\tilde{v}}$}
Let $\mathfrak{O}_{\tilde{v}}$ be a $n\times\delta$ matrix where $\mathfrak{O}_{\tilde{v}}(v,i)$ is the number of paths from $\tilde{v}$ to $v$ in the shortest path DAG $G_{\tilde{v}}$ with discrete length $i$.

\textbf{Example}\\
Using the graph illustrated in figure \ref{fig:shortest-path-dag}, we find that there are two paths from $a$ to $c$ in $G_{\tilde{a}}$ of discrete length 2 and 3 so $\mathfrak{O}_{a}(c,1)=0,\ \mathfrak{O}_{a}(c,2)=1,\ \mathfrak{O}_{a}(c,3)=1$.

The rows of $\mathfrak{O}_{\tilde{v}}$, denoted by $\mathfrak{o}_{\tilde{v}}^v$, are computed recursively using a message-passing algorithm starting at the root. Intuitively, there is always exactly one path from the root to itself in the shortest path DAG and this path is of discrete length 1 so $\mathfrak{o}_{\tilde{v}}^{\tilde{v}}=[1]$. From the current node, the algorithm passes a message to each of its reachable neighbours (children) containing its own row shifted one place to the right and adds the contents to the child's row. The algorithm then repeats on the next level of nodes. This is illustrated in figure \ref{fig:compute_o}.


\begin{figure}[H]
	\includegraphics[width=12cm]{compute_o}
	\caption{\textit{Computing $\mathfrak{O}_{\tilde{a}}$ using message-passing. Detailed in appendix \ref{appendix:illustrations_compute_o}.}}
	\label{fig:compute_o}
\end{figure}


Before defining the algorithm for computing $\mathfrak{O}_{\tilde{v}}$ we must define two constructs. First, let $\oplus$ be left-aligned addition between two vectors such that their length is aligned. I.e.:
\begin{equation}
[a, b] \oplus [c, d, e] = [a + c, b + d, e]
\end{equation}
Now, let $V_{\tilde{v}}^j$ be the nodes in $G_{\tilde{v}}$ such that there exists some path from the root to $v$ in $G_{\tilde{v}}$ with discrete length $j$. The algorithm is shown in figure \ref{algorithm:o}.

\begin{figure}[H]
\begin{lstlisting}
$\mathfrak{o}_{\tilde{v}}^{\tilde{v}}=[1]$
$\mathfrak{o}_{\tilde{v}}^v=[0], for\ v\in V \ {\tilde{v}}$
for $j=1..\delta$
   for $v\in V_{\tilde{V}}^j$
      for $w$ in $v$.children
         $\mathfrak{o}_{\tilde{v}}^w=\mathfrak{o}_{\tilde{v}}^w\oplus[0,\mathfrak{o}_{\tilde{v}}^v]$
      end
   end
end
\end{lstlisting}
\caption{Algorithm for computing $\mathfrak{O}_{\tilde{v}}$}
\label{algorithm:o}
\end{figure}

When computing $\mathfrak{O}_{\tilde{v}}$ each node sends exactly one message of size $\leq \delta$ to all of its children which are then updated. I.e.\ $|E_{\tilde{v}}|$ messages of size $\leq \delta$ giving a complexity of $\mathcal{O}(|E_{\tilde{v}}|\delta) \leq \mathcal{O}(|E|\delta)$.

\subsubsection{Computing $\mathfrak{D}_{\tilde{v}}$}

Let $\mathfrak{D}_{\tilde{v}}$ be a $n\times\delta$ matrix where $\mathfrak{D}_{\tilde{v}}(v, i)$ is the number of paths of discrete length $i$ in $G_{\tilde{v}}$ starting in $v$.

\textbf{Example}\\
Consider the node $d$ in figure \ref{fig:shortest-path-dag}. From this node there is one path in $G_{\tilde{a}}$ to itself of discrete length 1 ($\pi_1=[d]$) and one path to $c$ of discrete length 2 ($\pi_2=[d,c]$) so $\mathfrak{D}_a(d,1)=1,\ \mathfrak{D}_a(d,2)=1,\ \mathfrak{D}_a(d,i)=0$ for $i\notin\{1,2\}$.

The algorithm for computing $\mathfrak{D}_{\tilde{v}}$ will, like in the case of $\mathfrak{O}_{\tilde{v}}$, be using a message-passing approach. In this case the row for node $v$ in $\mathfrak{D}_{\tilde{v}}$ is denoted by $\mathfrak{d}_{\tilde{v}}^v$. We start by observing that all nodes must have exactly one path of discrete length 1 (the path to themselves) so we initialize $\mathfrak{d}_{\tilde{v}}^v=[1],\ \forall v\in V$. The $\mathfrak{d}_{\tilde{v}}^v$ row for $v$ is then computed by adding (using the $\oplus$ operation defined earlier) the rows of its children shifted one place to the right. The rows of the children are computed recursively until a leaf node is reached and the result is then send up through the stack of recursive calls. This is illustrated in figure \ref{fig:compute_d}.
\begin{figure}[H]
	\includegraphics[width=12cm]{compute_d}
	\caption{\textit{Computing $\mathfrak{d}_{\tilde{a}}$ using message-passing.  Detailed in appendix \ref{appendix:illustrations_compute_d}.}}
	\label{fig:compute_d}
\end{figure}

A recursive algorithm for computing $\mathfrak{D}_{\tilde{v}}$ can then be written out as in figure \ref{algorithm:d}.

\begin{figure}[H]
\begin{lstlisting}
function compute_d_rec(v)
   if not $\mathfrak{d}_{\tilde{v}}^v$.is_computed
      for $w$ in $v$.children
         $\mathfrak{d}_{\tilde{v}}^v = \mathfrak{d}_{\tilde{v}}^v \oplus $[0,compute_d_rec(w)]
      end
      $\mathfrak{d}_{\tilde{v}}^v$.is_computed = true
   end
   return $\mathfrak{d}_{\tilde{v}}^v$
end
	
initialize: $\mathfrak{d}_{\tilde{v}}^v=[1],\ \forall v\in V$
compute_d_rec($\tilde{v}$)
	
\end{lstlisting}
	\caption{Algorithm for computing $\mathfrak{D}_{\tilde{v}}$}
	\label{algorithm:d}
\end{figure}

When computing $\mathfrak{D}_{\tilde{v}}$ with the algorithm detailed in figure \ref{algorithm:d} a node visits each of its children in $G_{\tilde{v}}$ exactly once when computing its row and in this process their rows are computed recursively. Effectively, each edge $e\in E_{\tilde{v}}$ will be traversed once and each time a message will be returned of size $\leq \delta$ which updates the calling node. This gives a complexity of $\mathcal{O}(|E_{\tilde{v}}|\delta)\leq(|E|\delta)$


\subsubsection{Putting things together}
Having computed $\mathfrak{O}_{\tilde{v}}$ and $\mathfrak{D}_{\tilde{v}}$ we now observe that the number of times that $v$ appears as the $i$'th node in a path of discrete length $j$ in $G_{\tilde{v}}$ must be the number of paths from $\tilde{v}$ to $v$ in $G_{\tilde{v}}$ of length $i$ times the number of paths of length $j-i+1$ in $G_{\tilde{v}}$ starting in $v$; i.e.\ $\mathfrak{O}_{\tilde{v}}^v(v,i)\mathfrak{D}_{\tilde{v}}(v,j - i + 1)$.

\textbf{Example}\\
Consider the node $d$ in figure \ref{fig:shortest-path-dag}. Trying to find the number of paths in $G_{a}$ such that $d$ is the 2nd node in a path of length 3 we can intuitively see that there is one such path: $[a,d,c]$. This equivalent to the number of paths of length 2 from $a$ to $d$ times the number of paths of length $3-2+1=2$ starting in $d$.
This could be computed using the $\mathfrak{o}_a^d$ and $\mathfrak{d}_a^d$ computed earlier in figures \ref{fig:compute_o} and \ref{fig:compute_d}:
\begin{align}
\mathfrak{o}_a^d(2)\mathfrak{d}_a^d(3-2+1)&=\mathfrak{o}_a^d(2)\mathfrak{d}_a^d(2)\\
&=1 * 1\\
&=1
\end{align}

Being able to efficiently count the number of times $v$ appears as the $i$'th node in a path of length $j$ in a shortest path from $\tilde{v}$ we can now look back at the definition of $M(v)$ and conclude that

\begin{equation}
M(v)_{ij} = \sum_{\tilde{v}\in V}\mathfrak{o}_{\tilde{v}}^v(i)\mathfrak{d}_{\tilde{v}}^v(j-i+1)
\label{eq:runtime_comp_m}
\end{equation}

Computing $M$ may be achieved using the algorithm shown in figure \ref{alg:compute_M}. First we compute $G_{\tilde{v}},\mathfrak{O}_{\tilde{v}},\mathfrak{D}_{\tilde{v}}$ for all $\tilde{v}\in V$. Using a Dijkstra implementation\footnote{Using a binary-heap implementation as described in \cite{alg-bible} and assuming that all nodes are reachable.} with complexity $\mathcal{O}(m\log n)$ to compute $G_{\tilde{v}}$ this gives a complexity of $\mathcal{O}(n((m\log n) + m\delta + m\delta)) = \mathcal{O}(n((m\log n + m\delta))$. We then iterate over each pair of $v,\tilde{v}$ and $j=1..\delta,i=1..j$ giving a complexity of $\mathcal{O}(n^2\delta^2)$. Adding this together we get a total complexity of
\begin{align}
\mathcal{O}(n(m\log n + m\delta) + n^2\delta^2)&=\mathcal{O}(n(m\log n + m\delta + n\delta^2))
\end{align}

\begin{figure}[H]
\begin{lstlisting}
for $\tilde{v}\in V$
   compute $G_{\tilde{v}}$ using Dijkstra
   compute $\mathfrak{O}_{\tilde{v}}$
   compute $\mathfrak{D}_{\tilde{v}}$
end
let $M$ be a $n\times \delta \times \delta$ matrix of zeros
for $v \in V$
   for $\tilde{v}\in V$ 
      for $j\in 1..\delta$
         for $i\in 1..j$
            $M(v)_{ij}+=\mathfrak{o}_{\tilde{v}}^v(i)\mathfrak{d}_{\tilde{v}}^v(j-i+1)$
         end
      end
   end
end
\end{lstlisting}
\caption{Algorithm for computing M.}
\label{alg:compute_M}
\end{figure}

\subsection{Computational analysis}
\label{sec:computational_analysis}
To compute $k_{graphopper}$ we sum over each pair of nodes which, given precomputed $w(v,v')$ and $k_n(v,v')$, gives a complexity of $\mathcal{O}(n^2)$. Assuming that the complexity of computing $k_n(v,v')$ grows linearly relative to the dimension $d$ of the node attributes the complexity of computing all $k_n(v,v')$ has a complexity of $\mathcal{O}(n^2d)$. Computing the weight $w(v,v')$ between two nodes $v,v'$ (equation (\ref{eq:wmm})) consists of summing for $j\leq \delta,i\leq j$ giving $\mathcal{O}(\delta^2)$ and doing this for each pair gives a complexity of $\mathcal{O}(n^2\delta^2)$. Together this gives a complexity of $\mathcal{O}(n^2(d + \delta^2))$. Before doing these computations, however, we must first compute the $M$ matrices as described earlier (equation (\ref{eq:runtime_comp_m})) adding up to a total complexity of
\begin{align}
&\mathcal{O}\left(n^2\left(d + \delta^2\right) + n\left(m\log n + m\delta + n\delta^2\right)\right)\\
=\ &\mathcal{O}\left(n^2\left(d+\delta^2\right)+nm\left(\log n + \delta\right)\right)
\label{eq:k_one_pair}
\end{align}
for comparing two graphs where $n$ is the greatest number of nodes in a single graph, $m$ is the greatest number of edges in a single graph and $\delta$ is the greatest width of any graph. Typically we will not just be comparing two graphs. On the contrary, we will want to compute the inner product between all pairs of a large number of graphs. When doing so we can run the pairwise comparison more efficiently than described in (\ref{eq:k_one_pair}) as we may precompute all $M$ matrices before running the pairwise computations. Let $N$ be the number of graphs to compare. Then we must compute $N$ individual $M$ matrices. Using the complexity described in (\ref{eq:runtime_comp_m}) this gives a complexity of
\begin{equation}
\mathcal{O}\left(N\left(n\left(m\log n + m\delta + n\delta^2\right)\right)\right)
\end{equation}

We now have to compute $k_{graphhopper}$ on each unique pair of graphs. In practice, since a kernel is by definition symmetric, we do not consider all $N^2$ sequences. Instead, we count the number of multisets of cardinality 2 in the set of $N$ graphs, i.e.\ $\multiset{N}{2}$. This gives a complexity of
\begin{equation}
\mathcal{O}\left(\multiset{N}{2}\left(n^2\left(d+\delta^2\right)\right)\right)
\end{equation}
Adding the complexity of precomputing the $M$ matrices we land at a total complexity of
\begin{equation}
\mathcal{O}\left(\multiset{N}{2}\left(n^2\left(d+\delta^2\right)\right) + N\left(n\left(m\log n + m\delta + n\delta^2\right)\right)\right)
\end{equation}


\subsection{Implementation}
In this section we present the algorithm used in our implementation of GraphHopper described using pseudo-code. Based on the theory explained earlier we are able to effectively split the computation into two steps:
\begin{enumerate}
	\item Compute $M$ for all graphs.
	\item Compute $K$ using all pairs of M.
\end{enumerate}
The code presented here will not be a 1:1 match with the actual implementation but rather an introduction to the algorithm. We will exclude unneeded details including optimizations that are not required to understand the implementation.

\subsubsection{Computing M}

\begin{lstlisting}
#Computes the M matrix for a graph
function computeM($graph$){
   for $\tilde{v}\in graph.V$
      prepareNode($\tilde{v}$)
   end
   
   #Let M be an $|V|\times\delta\times\delta$ array of zeros
   $graph.M$ = array$[|graph.V|][graph.\delta][graph.\delta]$
   #Now sum the data of $\mathfrak{D}$ and $\mathfrak{O}$
   for $v\in graph.V$
      for $\tilde{v}\in graph.V$
         for $j\in 1..graph.\delta$
            for $i\in 1..j$
               $M[v][i][j] += \mathfrak{o}_{\tilde{v}}^v(i)\mathfrak{d}_{\tilde{v}}^v(j-i+1)$
            end
         end
      end
   end
}

#Computes $\mathfrak{O}_{\tilde{v}}$ and $\mathfrak{D}_{\tilde{v}}$
#Starts by creating $G_{\tilde{v}}$ using Dijkstra 
#before computing the matrices.
function prepareNode($graph$, $\tilde{v}$)
   #Prepare data for Dijkstra
   $queue = []$
   for $v\in graph.V$
      $v.d = \infty$
      $v.parents = []$
      $v.children = []$
      $queue.append(v)$
   end
   $\tilde{v}.d = 0$
   #$queue$ is a min heap allowing us to pop
   #the node with lowest d
   $queue.make\_min\_heap()$
   $\delta = 0$
   while not $queue.empty()$
      $u = queue.front()$
      $queue.pop()$
      
      $d = u.d + 1$
      for $v \in u.adjacent$
         if $v.d \geq d$
            if $v.d > d$
               $v.parents = []$
            end
            $v.parents.append(u)$
            $v.d = d$
            $\delta=max(\delta, d)$
         end
      end
      $queue.make\_min\_heap()$
   end
   #Set the graph's width to be the maximum width
   #found computing Dijkstra for any $\tilde{v}$
   $graph.\delta = max(graph.\delta, \delta)$
   
   #Putting nodes in buckets grouped and ordered
   #by shortest distance to $\tilde{v}$
   $\tilde{v}.ordered = $group_nodes($graph.V$)
   
   #Use $parents$ data to define $children$ in nodes
   prepare_children($graph.V$)
   
   #Refer to algorithms in last theory section
   computeO($\tilde{v},\ \delta$)
   computeD($\tilde{v}$)
end


\end{lstlisting}

\subsection{Gaps}
The following section is based on the description of GraphHopper with gaps on trees from \cite{gappy}. While this paper only describes the use of gaps on trees we will be expanding the definitions to also work on graphs in general.

When working on graph data (or string sequences, see \cite{string-kernels}) we may wish to identify paths that are very similar although they may contain small differences. These dissimilarities could be a result of many things depending on the given data e.g. mutations in DNA, missing data from image conversion etc. In the implementation detailed until now we have only been able to identify identical paths but this can be changed by allowing gaps. Introducing gaps allows us to create new paths called \textit{gappy paths} in the graphs which may skip up to $s$ nodes at a time (i.e.\ gaps of size $\leq s$). In figure \ref{fig:simple_gaps} is it shown how we can apply gaps to a simple graph.
\begin{figure}[H]
	\centering
	\includegraphics[width=12cm]{gaps}
	\caption{\textit{Left: No gaps. Middle: Gap size 1. Right: Gaps size 2.}}
	\label{fig:simple_gaps}
\end{figure}

Gappy paths are added to the shortest path DAG $G_{\tilde{v}}$ before computing $\mathfrak{O}_{\tilde{v}}$ and $\mathfrak{D}_{\tilde{v}}$. To add gappy paths we look at each pair of nodes $v,v'\in E_{\tilde{v}}$. We then add an edge $(v,v')$ to an updated set of edges $E'_{\tilde{v}}$ iff there exists a path in $G_{\tilde{v}}$ from $v$ to $v'$ of discrete length $\leq s + 2$. E.g. if there exists a path $\pi=[v_1,v_2,v_3]$ in $G_{\tilde{v}}$ and we allow gaps of size $s=1$ then the discrete length $|\pi| = 3 \leq s + 2 = 3$ so we add the edge $(v_1,v_2)$ to $E'_{\tilde{v}}$. This can be formalized as
\begin{equation}
\label{eq:gaps}
E'_{\tilde{v}} = E_{\tilde{v}} \cup \{ \forall(v, v')|\pi \in P_{G_{\tilde{v}}} \land \pi = [v,...,v'] \land |\pi| \leq s + 2 \}
\end{equation}

where $P_{G_{\tilde{v}}}$ is the set of all paths in $G_{\tilde{v}}$.

Adding gappy paths simply consists of adding new edges to the shortest path DAG. Afterwards we compute $M$ and eventually $K$ in the same way as we normally would as discussed in section \ref{section:graphhopper}.

\subsubsection{Computational analysis (continued)}
When we compute $k_{graphhopper}$ with gaps, the time complexity changes a little bit, the reason for this is that we need to add all the new edges to the graph. We do this by iterating through the root nodes parents in the shortest path graph. The time complexity for finding all these new edges are shown in (\ref{eq:tc_gaps_1}), where $\Delta$ is the maximum degree of the graph.
\begin{equation}
\label{eq:tc_gaps_1}
\mathcal{O}\left(n^2 \Delta^{s + 1}\right)
\end{equation}
The complexity shown in (\ref{eq:tc_gaps_1}) does not include adding the new edges to the graph, this can be done by iterating through the list of the nodes with the new parents and the list with the new children, where each step is $\Delta^s$.
\begin{equation}
\label{eq:tc_gaps_2}
\mathcal{O}\left(n^2 \Delta^{s}\right)
\end{equation}
The combined time complexity for (\ref{eq:tc_gaps_1}) and (\ref{eq:tc_gaps_2}) is $\mathcal{O}\left(n^2 \Delta^{s + 1}\right)$, but because we need to calculate this for every graph multiply them with $N$, the final time complexity for calculating gaps can be seen in (\ref{eq:tc_gaps_3}).
\begin{equation}
\label{eq:tc_gaps_3}
\mathcal{O}\left(N n^2 \Delta^{s + 1}\right)
\end{equation}
After calculating the time complexity for gaps, we can then combine our newly found time complexity, with the time complexity found in \ref{sec:computational_analysis}.
\begin{equation}
\mathcal{O}\left(N\left(n\left(m\log n + m\delta + n\delta^2 + n\Delta^{s + 1}\right)\right)\right)
\end{equation}
The action of adding gaps can look time consuming, because the time complexity does not look good, but when we compare the theoretical time with the actual time, we can see that the time complexity is a bit misleading. In figure \ref{fig:time_complexity_gaps} can a comparison be seen of the theoretical and actual time, the actual time is the time of calculating $M$, we did this a $100$ times and calculated the mean, to get a more realistic result.
\begin{figure}[H]
	\centering
	\includegraphics[width=10cm]{time_complexity_gaps}
	\caption{\textit{Fill out}}
	\label{fig:time_complexity_gaps}
\end{figure}
\subsubsection{Implementation}
In this section we will show how simple it is to add gaps to our current graphhopper implementation.
\begin{lstlisting}
function prepareNode($graph$, $\tilde{v}$)

#Dijkstra completed

  if $s > 0$
    for $v \in graph.V$
      #Find all the new parents and children, 
      #with a gap of size $1$..$s$.
      $current\_parents = []$
      $tmp\_parents = v.parents$
      for $i \in 1..s$
        $current\_parents = tmp\_parents$
        $tmp\_parents.clear()$
        for $j \in 1..current\_parents.size()$
          for $k \in 1..current\_parents[j].parents.size()$
            $v.grandParents.append(current\_parents[j].parents[k])$
            $current\_parents[j].parents[k].grandChildren.append(v)$
            $tmp\_parents.append(current\_parents[j].parents[k])$
          end
        end
      end

  #Move grandparents/grandchildren
  #to parents/children.
  for $v \in graph.V$
    while not $v.grandParents.empty()$
      $v.parents.append(v.grandParents.pop())$
    end
    while not $v.grandChildren.empty()$
      $v.children.append(v.grandChildren.pop())$
    end
  end

  #After adding the new paths, we can call:
  #computeO and computeD

end
\end{lstlisting}

\section{Node kernels}
\label{section:node-kernels}
As mentioned in section $\ref{section:graphhopper}$ one of the advantages of GraphHopper is that we apply any node kernel to compare the nodes. In this section we introduce the node kernels that we have chosen to include in our implementation. The following equations will be comparing two values $x,x'$ which when used as node kernel represent the labels of two nodes $v,v'$.

\subsection{Dirac}
Dirac is a simple kernel best suited for comparing discrete values representing non-continuous data, e.g. categorized nodes. The kernel returns 1 if the two values are equal and 0 otherwise. The kernel is formally described in (\ref{eq:dirac}).
\begin{equation}
\label{eq:dirac}
k_{dirac}(x, x')=\begin{cases}
1 & \text{for }x=x'\\
0 & \text{otherwise}
\end{cases}
\end{equation}

\subsection{Linear}
The linear node kernel is the dot product of $x$ and $x'$ as formally described in (\ref{eq:linear}). It is applicable to both discrete and continuous data.
\begin{equation}
\label{eq:linear}
k_{linear}(x, x') = x \cdot x'
\end{equation}

\subsection{Gaussian}
The Gaussian kernel is a popular choice for comparing continuous data. The kernel is formally defined in equation (\ref{eq:gaussian}). When two values are equal to each other the kernel returns 1 and the more they differ the closer the result gets to 0. By fine tuning the $\sigma$ parameter (e.g. by cross-validation) you may achieve a smooth curve that manages to describe the similarity of two values in a non-binary way by assigning a value between 0 and 1.

\begin{equation}
\label{eq:gaussian}
k_{gaussian}(x, x') = exp(-\frac{||x - x'||^2}{2\sigma^2})
\end{equation}

Our implementation is based on a compressed equation as defined in (\ref{eq:gaussian_simple}) where $\mu=\frac{1}{2\sigma^2}$. The two equations yield the same result but using $\mu$ as a parameter instead of $\sigma$ we save time by eliminating some calculations.

\begin{equation}
\label{eq:gaussian_simple}
k_{gaussian}(x, x') = exp(-\mu ||x - x'||^2)
\end{equation}

The curve of the Gaussian kernel is illustrated in figure \ref{fig:gaussian_graph} where the $x$ axis represents the difference between two values and the $y$ axis represents the result yielded by this Gaussian funciton. The width of the curve is controlled by the $\sigma$ parameter. When $\sigma$ grows the curve gets wider and as such the function becomes less sensitive to dissimilarity.

\begin{figure}[H]
	\centering
	\includegraphics[width=10cm]{gaussian}
	\caption{\textit{Illustration of the Gaussian kernel for $\sigma = 0.3$}}
	\label{fig:gaussian_graph}
\end{figure}


\subsection{Bridge}
The Bridge kernel\footnote{As defined in \cite{shortest-path}} as formalized in equation (\ref{eq:bridge}) is similar to the Gaussian kernel in that it yields a result between 0 (for very dissimilar values) and some constant $c>0$ for equal values. Instead of a smooth curve, however, this kernel returns $c$ minus the linear distance between the values although not less than 0. The $c$ constant is a parameter determining the maximum distance between a pair of values before they are regarded as non-similar and may be selected by cross-validation. Note that the Bridge kernel is only positive semidefinite for one-dimensional node data.

\begin{equation}
\label{eq:bridge}
k_{bridge}(x, x') = \text{max}(0, c - |(||x - x'||)|)
\end{equation}

The kernel is illustrated in figure \ref{fig:bridge_graph} .

\begin{figure}[H]
	\centering
	\includegraphics[width=10cm]{bridge}
	\caption{\textit{Illustration of the Bridge kernel for $c=1$}}
	\label{fig:bridge_graph}
\end{figure}

\subsection{Dirac $\times$ Gaussian}
Dirac $\times$ Gaussian is used to combine discrete non-continuous values with continuous values. Given a pair of tuples $(x,y),(x',y')$ where $x,x'$ are discrete categorized values and $y,y'$ are continuous values the kernel returns $k_{gaussian}(y,y')$ if $x=x'$ and 0 otherwise. This may be formalized as a product between the Dirac kernel and the Gaussian kernel as seen in equation (\ref{eq:diractimesgaussian}).
\begin{equation}
\label{eq:diractimesgaussian}
k_{diractimesgaussian}((x,y), (x',y')) = k_{dirac}(x, x') k_{gaussian}(y, y')
\end{equation}
Apart from possibly getting a higher accuracy than when using Gaussian alone this kernel may also be faster since we can skip computing the possibly heavy Gaussian product (depending on data dimensionality) when the discrete values are not equal and the extra task of determining equality of discrete values is very light in comparison.

\section{Product}
The product consists of a C++ application which may be compiled using a MEX-compiler and run directly in MATLAB. A manual for installation and use is provided in appendix \ref{appendix:manual}. We have chosen to leave out technical details of the implementation in this report but the source code is available online as described in the manual. The solution has been tested in great part by comparing our results to the results generated by an existing implementation in MATLAB available at

\textit{https://sites.google.com/site/aasaferagen/home/software} 

\subsection{Implemented functionality}
The application computes, utilising an arbitrary amount of threads, the GraphHopper kernel matrix on a set of graphs using a (possibly parameterized) node kernel and allowing gaps of size $s\geq 0$ as specified by the user. The implemented node kernels are detailed in section \ref{section:node-kernels}. It is mainly supposed to be compiled using a MEX-compiler and executed directly in MATLAB but may also be compiled as a standalone application using file-based in- and output.

The project also contains a MATLAB script for cross-validating node kernel parameters (extending the functionality of scripts supplied with \cite{graphhopper}\footnote{https://www.bsse.ethz.ch/mlcb/research/machine-learning/graph-kernels/scalable-kernels-for-graphs-with-continuos-attributes.html}) and a C++ application for translating .mat files into the file format accepted by the standalone GraphHopper application.

\subsection{Possible extensions}
Given more time, one could consider implementing multiple extensions to the current application.

As mentioned earlier we have supplied a simple MATLAB script for cross-validating node kernel parameters. This script works by running the GraphHopper application repeatedly each time recomputing $M$ before computing $K$ using the new node kernel parameter. By implementing this functionality inside the application itself we could compute multiple $K$ matrices reusing the same $M$ matrix. On the same note we could consider cross-validating the maximum gap size in the GraphHopper application.

Since the current application returns the $K$ matrix, the natural extension of cross-validation would be to return a list of $K$ matrices each paired with the parameters they were computed with (which may then be classified and cross-validated). As this would potentially require a massive amount of memory it should be considered starting the classification process from within the application itself.

\section{Experiments}
In this section we provide and discuss results based on computations made using our product. First we analyse the running time of our implementation compared to an implementation scripted in MATLAB. Later, we examine the effect on accuracy when adding gaps of different lengths.

\subsection{Datasets}
Basic specifications of the datasets used throughout our experiments are supplied in table \ref{table:data-sets}.

\begin{table}[H]
		\centering
		\hspace*{-0.7in}
		\scalebox{0.6}{
\begin{tabular}{c|c|c|c|c}
	Dataset & Number of graphs & Avg. nodes & Max nodes & Attr. dimensionality\\
	\hline
	Mutag  & 188 & 17.93 & 28 & 1\\
	Enzymes  & 600 & 32.63 & 126 & 1\\
	Enzymes (Symmetrized) & 600 & 32.63 & 126 & 1 + 18\\
	Airways subsampled & 500 & 221.31 & 651 & 1 \\
	NCI1 & 4110 & 29.87 & 111 & 1\\
	NCI109 & 4127 & 29.68 & 111 & 1 \\
	Reddit Binary & 2000 & 429.63 & 3782 & 1 \\
	DD & 1178 & 284.32 & 5748 & 1\\
	IMDB Binary & 1000 & 19.77 & 136 & 1\\
\end{tabular}}
	\caption{\textit{Dataset specifications}}
	\label{table:data-sets}
\end{table}

\subsubsection{Details}
\textbf{Mutag, Enzymes, NCI1, NCI109, DD} are all standard benchmark datasets based on chemoinformatics downloaded from ETH Zürich Department of Biosystems Science and Engineering\footnote{https://www.bsse.ethz.ch/mlcb}.

\textbf{Airways subsampled} consists of graph data converted from images of lung blood vessels. The task is to determine whether a given lung belongs to a healthy individual or one suffering from Chronic Obstructive Pulmonary Disease (COPD) \cite{graphhopper}.

\textbf{Reddit binary} \cite{yanardag} contains data from the social media website reddit.com. Each graph is a discussion thread belongning to one of four \textit{subreddits}\footnote{Discussion board limited to a specific subject.}. Two of these subreddits (\textit{IAmA} and \textit{AskReddit}) are classified as question/answer-based while the two others (\textit{TrollXChromosomes} and \textit{atheism}) are classified as discussion-based subreddits. Each node represents a user who participated in the given thread and there is an edge between any pair of users iff one of the users submitted a comment to any of the other user's comments. The node labels for a given node is a discrete value equal to the number of edges going to/from this node. The task is to determine whether a given thread belongs to a question/answer-based or discussion-based subreddit.

\textbf{IMDB Binary} \cite{yanardag} contains movie-related collaboration data. Each graph is an \textit{ego network} generated for an actor/actress. Each node represents an actor and there is an edge between two nodes if they have appeared in the same movie. Each ego network belongs to a certain movie genre (comedy or romance) and the task it to identify which genre a given network belongs to.

\subsection{Runtime experiments (C++ vs MATLAB)}
In this section we wish to see whether we succeeded in creating a faster implementation using C++ compared to the existing implementation. The results show the running time in seconds when computing the $K$ matrix for a specified dataset with both implementations using the same node kernel as specified and without gaps. All tests are run without parallelization.

\subsubsection{System specifications}
The computations have been done in various environments as specified in table \ref{table:specs}.
\begin{table}[H]
	\centering
	\hspace*{-0.7in}
	\scalebox{0.7} {
		\begin{tabular}{c|c|c|c}
			ID & OS & CPU & Ram\\
			\hline
			A & Win7 64 bit & Intel i5-3320M @ 2.60GHz (4 cores) & 8GB \\
			\hline
			B & Win10 64 bit & AMD FX-8150 Black Edition @ 3.60GHz (8 cores) & 16GB
			
		\end{tabular}
	}
	
	\caption{\textit{System specifications}}
	\label{table:specs}
\end{table}

\subsubsection{Results}
The results are presented in table \ref{table:runtime_results}. The \textit{system spec} column refers to the specifications presented in table \ref{table:specs}.
\begin{table}[H]
	\centering
	\hspace*{-0.7in}
	\scalebox{0.7} {
		\begin{tabular}{c|c|c|c|c|c}
			Dataset & Node kernel & Runtime in MATLAB & Runtime in Mex C++ & System spec & Ratio\\
			\hline
			Mutag & Dirac & 31.6 & \textbf{0.731} & A & 1:43.23\\
			Enzymes & Dirac & 722.99 & \textbf{16.9} & A & 1:42.78\\
			Enzymes (Symmetrized) & Gaussian & 256.14 & \textbf{26.55} & A & 1:9.65 \\ 
			Enzymes (Symmetrized) & Dirac $\times$ Gaussian & 304.37  & \textbf{13.33} & A & 1:22.83\\
			Airways subsampled & Gaussian & 5518.01 & \textbf{2608.45} & A & 1:2.12 \\
			NCI1 & Dirac & 62617.89 & \textbf{1695.35} & B & 1:36.94\\
			NCI109 & Dirac & & \textbf{1831.39} & B & 
		\end{tabular}
	}
	\caption{\textit{Runtime results}}
	\label{table:runtime_results}
\end{table}

\subsubsection{Discussion}
We have achieved superior running times in all of our tests seemingly varying mainly on the choice of node kernel but also the size of the datasets. The greatest difference is achieved when using \textbf{Dirac} where our implementation runs about 40 times faster reaching a 1:43.23 ratio on a very small dataset and 1:36.94 on a larger dataset. When using \textbf{Gaussian} the gain is significantly smaller as we only reach from 1:2.12 on a large dataset up to 1:9.65 on a small dataset. Comparing the tests on Enzymes (Symmetrized) with Gaussian vs. with \textbf{Dirac $\times$ Gaussian} we notice that while our implementation manages to \textbf{half} the running time by using Dirac $\times$ Gaussian the MATLAB implementation requires even more computations than when running Gaussian alone.

\subsection{Accuracy experiments (Gaps vs. no Gaps)}
The accuracy experiments are run using 10-fold cross-validation of $c$ (slack variable constant, see \cite{svm-graph-kernels}) as well as kernel parameters where applicable (e.g.\ when using Gaussian). For reference we also provide our own results (some new, some from \cite{svm-graph-kernels}) with the established Weisfeiler-Lehman kernel\cite{WL} (WL) and (where available) results published in \cite{yanardag} using the proposed Deep Graphlet Kernel (DGK). Some of the datasets from \cite{yanardag} have been tested both with and without node labels and are in both cases compared to the same result from \cite{yanardag} since DGK does not use node labels either way.

\subsubsection{Results}
The results here are divided by datasets. The graphs illustrate the effect of changing the gap size in GraphHopper (GH) and at the same time display results from other kernels for reference. The node kernels used are abbreviated as D for Dirac, G for Gaussian and DxG for Dirac $\times$ Gaussian.

\textbf{Mutag}\\
\begin{minipage}{0.6\linewidth}
		\hspace*{-1in}
		\includegraphics[width=10cm]{mutag}
		\captionof{figure}{Mutag results}
		\label{fig:mutag}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
	
	\centering
	\begin{tabular}{c|c|c|c}
			Kernel & Node kernel & Gaps &  Result\\
			\hline
			GH & D & 0 & 83.111 (std. 1.9633)\\
			GH & D & 1 & 79.00 (std. 2.2801)\\
			GH & D & 2 & 79.3889 (std. 2.0861)\\
			GH & D & 3 &  77.3333 (std. 1.2776)\\
			WL & - & - & 83.611 (std. 2.3607)\\
			DGK & - & - & 82.66 (std. 1.45)
		\end{tabular}
	\captionof{table}{Mutag results}
	\label{table:mutag}
\end{minipage}

\textbf{Enzymes}\\
\begin{minipage}{0.6\linewidth}
	\hspace*{-1in}
	\includegraphics[width=10cm]{enzymes}
	\captionof{figure}{Enzymes results}
	\label{fig:enzymes}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
	
	\centering
	\begin{tabular}{c|c|c|c}
		Kernel & Node kernel & Gaps & Result\\
		\hline
		GH & D & 0 & 35.133 (std. 2.050)\\
		GH & D & 1 & 36.350 (std. 1.388)\\
		GH & D & 2 & 34.833 (std. 1.317)\\
		GH & D & 3 & 33.5667 (std. 1.4911)\\
		WL & - & - & 52.4167 (std. 1.2478)\\
		DGK & - & - & 27.08 (std. 0.79)
	\end{tabular}
	\captionof{table}{Enzymes results}
	\label{table:enzymes}
\end{minipage}

\textbf{Enzymes (Symmetrized)}\\
\begin{minipage}{0.6\linewidth}
	\hspace*{-1in}
	\includegraphics[width=10cm]{enzymes_symmetrized}
	\captionof{figure}{Enzymes (symmetrized) results}
	\label{fig:enzymes_symmetrized}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
	
	\centering
	\begin{tabular}{c|c|c|c}
		Kernel & Node kernel & Gaps & Result\\
		\hline
		GH & G & 0 & 68.5833 (std. 1.6125)\\
		GH & G & 1 & 68.5333 (std. 1.3352)\\
		GH & G & 2 & 68.9500 (std. 0.9783)\\
		GH & G & 3 & 68.8333 (std. 1.3744)\\
		GH & G & 4 & 69.7500 (std. 1.3591)\\
		GH & G & 5 & 68.8667 (std. 1.4673)\\
		GH & DxG & 0 & 70.033 (std. 1.2977)\\
		GH & DxG & 1 & 69.3167 (std. 1.1506)\\
		GH & DxG & 2 & 70.5833 (std. 1.3293)\\
		GH & DxG & 3 & 70.7000 (std. 0.9356)\\
		GH & DxG & 4 & 70.5167 (std. 1.2582)\\
		GH & DxG & 5 & 70.7167 (std. 0.9164)\\
		WL & - & - & 52.1000 (std. 1.1282)
	\end{tabular}
	\captionof{table}{Enzymes results}
	\label{table:enzymes_symmetrized}
\end{minipage}

\textbf{Airways subsampled}\\
\begin{minipage}{0.6\linewidth}
	\hspace*{-1in}
	\includegraphics[width=10cm]{airways}
	\captionof{figure}{Airways subsampled results}
	\label{fig:airways}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
	
	\centering
	\begin{tabular}{c|c|c|c}
		Kernel & Node kernel & Gaps & Result\\
		\hline
		GH & G & 0 & 63.180 (std. 1.7900)\\
		GH & G & 1 & 62.600 (std. 1.3233)\\
		GH & G & 2 & 61.6800 (std. 2.0422)\\
		WL & - & - & 59.48 (std. 1.7080)
	\end{tabular}
	\captionof{table}{Airways subsampled results}
	\label{table:airways}
\end{minipage}

\textbf{NCI1}\\
\begin{minipage}{0.6\linewidth}
	\hspace*{-1in}
	\includegraphics[width=10cm]{nci1}
	\captionof{figure}{NCI1 results}
	\label{fig:nci1}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
	
	\centering
	\begin{tabular}{c|c|c|c}
		Kernel & Node kernel & Gaps & Result\\
		\hline
		GH & D & 0 & 72.7908 (std. 0.2457)\\
		GH & D & 1 & 67.8589 (std. 0.2417)\\
		GH & D & 2 & 64.4161 (std. 0.6426)\\
		GH & D & 3 & 64.9903 (std. 0.5475)\\
		GH & D & 4 & 64.3698 (std. 0.4042)\\
		GH & D & 5 & 64.4988 (std. 0.4535)\\
		WL & - & - & 85.0096 (std. 0.1907)\\
		DKG & - & - & 62.48 (std. 0.25)
	\end{tabular}
	\captionof{table}{NCI1 results}
	\label{table:nci1}
\end{minipage}

\textbf{NCI109}\\
\begin{minipage}{0.6\linewidth}
	\hspace*{-1in}
	\includegraphics[width=10cm]{nci109} 
	\captionof{figure}{NCI109 results}
	\label{fig:nci109}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
	
	\centering
	\begin{tabular}{c|c|c|c}
		Kernel & Node kernel & Gaps & Result\\
		\hline
		GH & D & 0 & 71.0874 (std. 0.3168)\\
		GH & D & 1 & 66.5413 (std. 0.2810)\\
		GH & D & 2 & 64.3689 (std. 0.5451)\\
		GH & D & 3 & 63.9951 (std. 0.5259)\\
		WL & - & - & 85.1845 (std. 0.2438)\\
		DGK & - & - & 62.69 (std. 0.23)
	\end{tabular}
	\captionof{table}{NCI109 results}
	\label{table:nci109}
\end{minipage}


\textbf{Reddit binary (unlabelled)}\\
\begin{minipage}{0.6\linewidth}
	\hspace*{-1in}
	\iffalse	\includegraphics[width=10cm]{reddit_binary_unlabelled} \fi
	\captionof{figure}{Reddit binary (unlabelled) results}
	\label{fig:reddit_binary_unlabelled}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
	
	\centering
	\begin{tabular}{c|c|c|c}
		Kernel & Node kernel & Gaps & Result\\
		\hline
		GH & D & 0 & 100 (std. 0.0000)\\
		GH & D & 1 & 99.9500 (std. $<$ 0.0001)\\
		WL & - & - & \\
		DGK & - & - & 78.04 (std. 0.39)
	\end{tabular}
	\captionof{table}{Reddit binary (unlabelled) results}
	\label{table:reddit_binary_unlabelled}
\end{minipage}


\textbf{Collab (labelled)}\\
\begin{minipage}{0.6\linewidth}
	\hspace*{-1in}
	\iffalse	\includegraphics[width=10cm]{collab_labelled} \fi
	\captionof{figure}{Collab (labelled) results}
	\label{fig:collab}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
	
	\centering
	\begin{tabular}{c|c|c|c}
		Kernel & Node kernel & Gaps & Result\\
		\hline
		GH & Dirac & 0 & 78.6580 (std. 0.2647)\\
		GH & Dirac & 1 & 78.8960 (std. 0.2084)\\
		GH & Dirac & 2 & 79.0620 (std. 0.2461)\\
		GH & Dirac & 3 & 78.9980 (std. 0.3615)\\
		WL & - & - & \\
		DGK & - & - & 73.09 (std. 0.25)\\
	\end{tabular}
	\captionof{table}{Collab (labelled) results}
	\label{table:collab}
\end{minipage}

\textbf{Collab (unlabelled)}\\
\begin{minipage}{0.6\linewidth}
	\hspace*{-1in}
	\iffalse	\includegraphics[width=10cm]{collab_unlabelled} \fi
	\captionof{figure}{Collab (unlabelled) results}
	\label{fig:collab_unlabelled}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
	
	\centering
	\begin{tabular}{c|c|c|c}
		Kernel & Node kernel & Gaps & Result\\
		\hline
		GH & Dirac & 0 & 63.3100 (std. 0.0750) \\
		GH & Dirac & 1 & 63.1900 (std. 0.1595) \\
		GH & Dirac & 2 & 63.1560 (std. 0.1964) \\
		GH & Dirac & 3 & 63.1400 (std. 0.2022) \\
		WL & - & - & \\
		DGK & - & - & 73.09 (std. 0.25)
	\end{tabular}
	\captionof{table}{Collab (unlabelled) results}
	\label{table:collab_unlabelled}
\end{minipage}


\textbf{IMDB Binary (labelled)}\\
\begin{minipage}{0.6\linewidth}
	\hspace*{-1in}
	\includegraphics[width=10cm]{imdb_labelled}
	\captionof{figure}{IMDB Binary (labelled) results}
	\label{fig:imdb_labelled}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}	
	\centering
	\begin{tabular}{c|c|c|c}
		Kernel & Node kernel & Gaps & Result\\
		\hline
		GH & D & 0 & 71.37 (std. 0.8486)\\
		GH & D & 1 & 71.89 (std. 0.7824)\\
		GH & D & 2 & 71.69 (std. 1.0989)\\
		GH & D & 5 & 72.53 (std. 0.5229)\\
		GH & D & 7 & 72.25 (std. 0.8567)\\
		GH & D & 10 & 71.71 (std. 1.3552)\\
		GH & D & 15 & 71.90 (std. 0.2712)\\
		GH & D & 20 & 72.24 (std. 1.1787)\\
		GH & D & 25 & 71.72 (std. 1.3815)\\
		GH & D & 50 & 71.76 (std. 1.0362)\\
		WL & - & - & 73.39 (std. 0.5567) \\
		DGK & - & - & 66.96 (std. 0.56)\\
	\end{tabular}
	\captionof{table}{IMDB Binary (labelled) results}
	\label{table:imdb_labelled}
\end{minipage}

\textbf{IMDB Binary (unlabelled)}\\
\begin{minipage}{0.6\linewidth}
	\hspace*{-1in}
	\includegraphics[width=10cm]{imdb_unlabelled}
	\captionof{figure}{IMDB Binary (unlabelled) results}
	\label{fig:imdb_unlabelled}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}	
	\centering
	\begin{tabular}{c|c|c|c}
		Kernel & Node kernel & Gaps & Result\\
		\hline
		GH & D & 0 & 61.79 (std. 1.4933)\\
		GH & D & 1 & 58.46 (std. 1.3023)\\
		GH & D & 2 & 58.09 (std. 1.5022)\\
		GH & D & 3 & 58.17 (std. 1.5181)\\
		GH & D & 5 & 57.72 (std. 1.7498)\\
		GH & D & 10 & 59.56 (std. 1.3574)\\
		GH & D & 25 & 59.48 (std. 1.3726)\\
		GH & D & 50 & 58.14 (std. 2.2907)\\
		WL & - & - & 72.90 (std. 0.68961)\\
		DGK & - & - & 66.96 (std. 0.56) \\
	\end{tabular}
	\captionof{table}{IMDB Binary (unlabelled) results}
	\label{table:imdb_unlabelled}
\end{minipage}

\textbf{IMDB Multi (labelled)}\\
\begin{minipage}{0.6\linewidth}
	\hspace*{-1in}
	\iffalse	\includegraphics[width=10cm]{imdb_multi_labelled} \fi
	\captionof{figure}{IMDB Multi (labelled) results}
	\label{fig:imdb_multi_labelled}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}	
	\centering
	\begin{tabular}{c|c|c|c}
		Kernel & Node kernel & Gaps & Result\\
		\hline
		GH & Dirac & 0 & 50.3133 (std. 0.5938) \\
		GH & Dirac & 1 & 50.3133 (std. 0.5297) \\
		GH & Dirac & 2 & 50.6133 (std. 0.6739) \\
		GH & Dirac & 3 & 50.2067 (std. 0.5043) \\
		WL & - & - & \\
		DGK & - & - & \\
	\end{tabular}
	\captionof{table}{IMDB Multi (labelled) results}
	\label{table:imdb_multi_labelled}
\end{minipage}

\textbf{IMDB Multi (unlabelled)}\\
\begin{minipage}{0.6\linewidth}
	\hspace*{-1in}
	\iffalse	\includegraphics[width=10cm]{imdb_multi_unlabelled} \fi
	\captionof{figure}{IMDB Multi (unlabelled) results}
	\label{fig:imdb_multi_unlabelled}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}	
	\centering
	\begin{tabular}{c|c|c|c}
		Kernel & Node kernel & Gaps & Result\\
		\hline
		GH & Dirac & 0 & 40.3600 (std. 0.9889) \\
		GH & Dirac & 1 & 39.8467 (std. 0.8833) \\
		GH & Dirac & 2 & 39.8267 (std. 0.7438) \\
		GH & Dirac & 3 & 39.2933 (std. 0.7168) \\
	\end{tabular}
	\captionof{table}{IMDB Multi (unlabelled) results}
	\label{table:imdb_multi_unlabelled}
\end{minipage}


\textbf{SEMI}\\
\begin{minipage}{0.6\linewidth}
	\hspace*{-1in}
	\iffalse	\includegraphics[width=10cm]{semi} \fi
	\captionof{figure}{SEMI results}
	\label{fig:semi}
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}	
	\centering
	\begin{tabular}{c|c|c|c}
		Kernel & Node kernel & Gaps & Result\\
		\hline
		GH & DxG & 0 & 81.00 (std. 1.7033)
	\end{tabular}
	\captionof{table}{SEMI results}
	\label{table:semi}
\end{minipage}



\begin{table}[H]
	\centering
	\hspace*{-0.7in}
	\scalebox{0.6} {
		\begin{tabular}{c|c|c|c|c|c|c|c}
		Dataset & Node kernel & Without gaps & With gaps (size 1) & Gap size 2 & Gap size 3 & Gap size 4 & Gap size 5\\
		\hline
		Mutag & Dirac & $\mathbf{83.111 (std. 1.9633)}$ & $79.000 (std. 2.2801)$ & 79.3889 (std. 2.0861) & 77.3333 (std. 1.2776) & & \\
		Enzymes & Dirac & $\mathbf{35.133 (std. 2.050)}$ & $\mathbf{36.350 (std. 1.3888)}$ & 34.8333 (std. 1.3170) & 33.5667 (std. 1.4911) & & \\
		Enzymes (symmetrized) & Gaussian & $\mathbf{68.5833 (std. 1.6125)}$ & $\mathbf{68.5333 (std. 1.3352)}$ & 68.9500 (std. 0.9783) & 68.8333 (std. 1.3744) & 69.7500 (std. 1.3591) & 68.8667 (std. 1.4673) \\
		Enzymes (symmetrized) & Dirac $\times$ Gaussian & 70.033 (std. 1.2977) & 69.3167 (std. 1.1506) & 70.5833 (std. 1.3293) & 70.7000 (std. 0.9356) & 70.5167 (std. 1.2582) & 70.7167 (std. 0.9164)\\
		Airways subsampled & Gaussian & $\mathbf{63.180 (std. 1.7900)}$ & $\mathbf{62.600 (std. 1.3233)}$ & 61.6800 (std. 2.0422) & & & 61.9800 (std. 1.8961) \\
		NCI1 & Dirac & 72.7908 (std. 0.2457) & 67.8589 (std. 0.2417) & 64.4161 (std. 0.6426) & 64.9903 (std. 0.5475) & 64.3698 (std. 0.4042) & 64.4988 (std. 0.4535) \\
		NCI109 & Dirac & 71.0874 (std. 0.3168) & 66.5413 (std. 0.2810) & 64.3689 (std. 0.5451) & 63.9951 (std. 0.5259) & & \\
		Reddit binary (unlabelled) & Dirac & 100.000 (std. 0.0000) & 99.9500 (std. $<$ 0.001) & & & &  \\
	
		Collab (labelled) & Dirac & 78.6580 (std. 0.2647) & 78.8960 (std. 0.2084) & 79.0620 (std. 0.2461) & 78.9980 (std.0.3615 ) & \\
		Collab (unlabelled) & Dirac & 63.3100 (std. 0.0750) & & & & & \\
		IMDB Binary (labelled) & Dirac & 71.37 (std. 0.8486) & 71.89 (std. 0.7824) & & & & 72.53 (std. 0.5229) \\
		IMDB Binary (unlabeled) & 
		


		\end{tabular}
	}
	\caption{\textit{Accuracy results}}
	\label{table:raccuracy_results}
\end{table}
Enzymes (Symmetrized) \\
\iffalse
Gap size 10: 70.5833 (std. 2.2515) \\
Gap size 13: 69.9500 (std. 1.2274) \\
Gap size 15: 70.8167 (std. 1.1479) \\
\fi
Gap size 20: 70.4333 (std. 0.9336) \\
\iffalse
Gap size 21: 69.4167 (std. 1.5321) \\
Gap size 25: 69.7833 (std. 1.4825) \\
Gap size 30: 70.6333 (std. 1.4694) \\
Gap size 40: 69.4333 (std. 1.5480) \\
Gap size 50: 69.7833 (std. 1.4825)
\fi
\subsubsection{Discussion}

\section{Conclusion}



\newpage

\renewcommand\refname{References}
\begin{thebibliography}{9}
		\bibitem{graphhopper}
		Aasa Feragen, Niklas Kasenburg, Jens Petersen, Marleen de Bruijne, Karsten M. Borgwardt.
		\emph{Scalable kernels for graphs with continuos attributes}.
		In Advances in Neural Information Processing Systems, pages 216-224, 2013.

		\bibitem{gappy}
		Aasa Feragen, Karsten Borgwardt.
		\emph{Efficient tree-path kernels for attributed trees}.
		Not published yet.

		\bibitem{shortest-path}
		Karsten M. Borgwardt, Hans-Peter Kriegel.
		\emph{Shortest-path kernels on graphs}.
		In Proceedings of the International Conference on Data Mining, pages 74-81, 2005.
		
		\bibitem{svm-graph-kernels}
		Rune Kok Nielsen, Andreas Nugaard Holm.
		\emph{Afløsningsopgave i fagområdet Modellering og Analyse af Data}.
		University of Copenhagen, 2016\\
		https://drive.google.com/file/d/0BxPtSV8mme0xWGV0ZlpKRlFUU0U/view
		
		\bibitem{alg-bible}
		Thomas H. Cormen, Charles E. Lesierson, Ronald L. Rivest, Clifford Stein.
		\emph{Introduction to Algorithms 3rd ed.}.
		MIT Press, 2009.
		
		\bibitem{string-kernels}
		Christina Leslie, Rui Kuang.
		\emph{Fast String Kernels using Inexact Matching for Protein Sequences}.
		Columbia University, 2004
		
		\bibitem{yanardag}
		Pinar Yanardag, SVN Vishwanathan.
		\emph{Deep Graph Kernels}.
		Purdue University \& UC Santa Cruz, http://web.ics.purdue.edu/$\sim$ypinar/kdd/
		
		\bibitem{svn-introduction}
		Nello Christianini, John Shawe-Taylor.
		\emph{An introduction to Support Vector Machines and other Kernel-based Learning methods}.
		Cambridge University Press, 2000.
		
		\bibitem{WL}
		Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt
		Mehlhorn, Karsten M. Borgwardt.
		\emph{Weisfeiler-lehman graph kernels}.
		Journal	of Machine Learning Research, 12:2539-2561, 2011.
		
\end{thebibliography}


\newpage
\begin{appendices}
\section{Illustration comments}
\label{appendix:illustrations}
This section serves to explain the details of illustrations used in the report itself that may not be self-explanatory.
\subsection{Figure \ref{fig:compute_o}}
\label{appendix:illustrations_compute_o}
Reading left to right, top to bottom, the steps are as follows:
\begin{enumerate}
	\item The root sends a message to its children.
	\item The children's values are updated.
	\item The only non-leaf node in the next level is $d$, so it sends a message to its only child, $c$.
	\item The value of $c$ is updated. The only node in the next level is $c$, which is a leaf node, so the algorithm ends.
\end{enumerate}


\subsection{Figure \ref{fig:compute_d}}
\label{appendix:illustrations_compute_d}
Reading left to right, top to bottom, the steps are as follows:
\begin{enumerate}
	\item The algorithm starts at the root node $a$.
	\item The node $a$ asks $b$. Since $b$ is a leaf node it does not need to do any further work so it returns $[0, 1]$.
	\item The result from $b$ is added to $a$: $[1] \oplus [0,1] = [1,1]$.
	\item The node $a$ asks $c$. Like $b$, $c$ is a leaf node and returns $[0,1]$.
	\item The result is added to $a$: $[1,1] \oplus [0,1] = [1,2]$.
	\item The node $a$ asks $d$. Since $d$ is not a leaf node it must first determine its own value before returning.
	\item The node $d$ asks $c$ which returns $[0,1]$.
	\item The result from $c$ is added to $d$: $[1] \oplus [0,1] = [1,1]$ and $d$ returns $[0,1,1]$.
	\item The result from $d$ is added to $a$: $[1,2]\oplus[0,1,1]=[1,3,1]$. The root node has now collected the values of all its children so the algorithm ends.
\end{enumerate}

\newpage
\section{GraphHopper Software Manual}
\label{appendix:manual}
\subsection{Preface}
This manual guides the reader through installation and use of the software product created in the \textit{GraphHopper kernel in C++} project by Rune Kok Nielsen and Andreas Holm at the University of Copenhagen.
\subsection{Software purpose}
The product consists of an optimized implementation of the GraphHopper kernel written in C++ which can be run from the MATLAB environment by using a MEX C++ compiler. The software may effectively be used together with libSVM\footnote{https://www.csie.ntu.edu.tw/$\sim$cjlin/libsvm/} and the supplied MATLAB script for cross-validating kernel parameters.

\subsection{Installation}
\subsubsection{Prerequisites}
Interfacing with MATLAB requires a working MATLAB-installation, a C++ compiler supported by said installation and a functioning MEX-compilation environment. Read more about supported compilers at

\textit{http://mathworks.com/support/compilers/}
\subsubsection{Getting the project}
The project is hosted at

\textit{https://github.com/RuneKokNielsen/graphhopper}

You may either clone the project using a Git client or download the project as a compressed archive.

\subsubsection{Compiling the source code}
The compilation depends on the system itself but could be as simple as running the MEX-compiler on the \textit{mex\_main.cpp}. Let \verb|ROOT| be the root of the project. Then the file could be compiled as follows:
\begin{verbatim}
cd ROOT/src/graphhopper
mex -output graphhopper mex_main.cpp
\end{verbatim}
The \verb|-output| parameter is not necessary but saves you the trouble of renaming the resulting MEX-file. \textbf{IMPORTANT:} On 64-bit systems you must supply the \verb|-largeArrayDims| parameter to the compilation. Failing to do so will result in fatal runtime crashes. E.g.:
\begin{verbatim}
mex -output graphhopper -largeArrayDims mex_main.cpp
\end{verbatim}
If you are compiling on Win64 you may find inspiration in the makefile located in \verb|ROOT/src/win/makefile|. The resulting MEX-file can be run directly from MATLAB.

\subsubsection{Calling the software from MATLAB}
Assuming that the MEX-file is now accessible from MATLAB you may execute GraphHopper as a function named like the MEX-file. If you used the \verb|-output graphhopper| parameter during compilation the file should be named \verb|graphopper.[mex-extension]| and be callable as a function named \verb|graphhopper| in MATLAB. The function takes the following 6 parameters:
\begin{enumerate}
	\item \textbf{Graphs}: A variable containing the graphs to be compared. The variable must be formated as described in \ref{appendix:graphs-format}.
	\item \textbf{Node kernel}: A string describing which node kernel to use. The options are:
	\begin{itemize}
		\item \textbf{dirac}: Dirac node kernel. Only discrete node labels.
		\item \textbf{linear}: Linear node kernel. Works on discrete or vector node labels.
		\item \textbf{gaussian}: Gaussian node kernel. Works on discrete or vector node labels.
		\item \textbf{bridge}: Bride node kernel. Works on discrete or vector node labels.
		\item \textbf{diracxgaussian}: Dirac times Gaussian node kernel. Works on both discrete \textbf{and} vector node labels at the same time.
	\end{itemize}
	\item \textbf{Node labels}: A string describing which node labels to use. The options are:
	\begin{itemize}
		\item \textbf{discrete}: Loads the discrete labels in \verb|nl.values|.
		\item \textbf{vector}: Loads the vector labels in \verb|nl.vecvalues|.
		\item \textbf{both}: Loads both the discrete labels in \verb|nl.values| and the vector labels in \verb|nl.vecvalues|.
	\end{itemize}
	\item \textbf{Node kernel parameter}: A double value used in some node kernels (e.g. $\mu$ in Gaussian).
	\item \textbf{Gap size}: A double value describing the max gap size. Use \textit{0} (or drop fifth parameter) to run without gaps.
	\item \textbf{Threads}: The number of threads to run. Allows properly utilising multicore processors. Default is 1 (if less than 6 parameters are given).
\end{enumerate}

\textbf{Example calls}\\
Running Gaussian with $\mu=0.5$ and gap size $s=3$ on a single thread:\\
\verb|graphhopper(graphs, 'gaussian', 'vector', 0.5, 3)|

Running Dirac on 4 threads with gaps size $s=3$ (note that the value of the node kernel parameter is not used):\\
\verb|graphhopper(graphs, 'dirac', 'discrete', 0, 3, 4)|

Running Dirac $\times$ Gaussian on a single thread with $\mu=0.5$ and without gaps:\\
\verb|graphhoper(graphs, 'diracxgaussian', 'both', 0.5)|


\subsubsection{Graphs format}
\label{appendix:graphs-format}
The graphs variable must be a $1\times N$ struct (where $N$ is the number of graphs) containing a column labeled \verb|am| and a column labeled \verb|nl|. A row in the \verb|am| column should be a (full or sparse) adjacency matrix. A row in the \verb|nl| column should be a struct containing either a field named \verb|values| with discrete values if using discrete node labels and/or a field named \verb|vecvalues| if using vector node labels or both if using both node labels.

\end{appendices}

\end{document}